{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bikramkhanal/miniconda3/envs/tf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "from transformers import BertTokenizer #, BertForSequenceClassification, DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import itertools\n",
    "from tensorflow.nn import softmax\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import pandas as pd\n",
    "# import ast\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import balanced_accuracy_score,f1_score,accuracy_score #pip install -U scikit-learn\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 17:54:28.638674: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-01-31 17:54:28.638702: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-01-31 17:54:28.638709: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-01-31 17:54:28.638981: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-01-31 17:54:28.639410: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Some layers from the model checkpoint at model.h5 were not used when initializing TFBertForSequenceClassification: ['dropout_75']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load the pretrained model\n",
    "model = TFBertForSequenceClassification.from_pretrained('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the test data\n",
    "data = []\n",
    "with open(\"test.csv\", 'r') as csvfile:\n",
    "    for line in csvfile:\n",
    "        if len(line) > 5:\n",
    "            a, b = line.split('\",', 1)\n",
    "            a, b = a[2:], b[2:-2]\n",
    "            bs = re.findall(r\"'(.*?)'\", b, re.DOTALL)\n",
    "            bs = \", \".join(bs)\n",
    "            data.append([a,bs])\n",
    "# data = data[:10000]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the data into the text and the labels\n",
    "texts = [i[0] for i in data]\n",
    "labels = [i[1] for i in data]\n",
    "real_labels = [i[1] for i in data]\n",
    "#tokenize\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "input_ids = [tokenizer.encode(c,add_special_tokens=True) for c in texts]\n",
    "#pad\n",
    "input_ids = pad_sequences(input_ids, maxlen=256, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "# Initialze the label encoder\n",
    "le = LabelEncoder()\n",
    "# Fit the label encoder\n",
    "le.fit(labels)\n",
    "# Transform the labels\n",
    "labels = le.transform(labels)\n",
    "# One hot encode the labels\n",
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "preds = model.predict(input_ids)\n",
    "logits = preds.logits\n",
    "# get the predicted labels\n",
    "preds = np.argmax(logits, axis=1)\n",
    "# get the accuracy\n",
    "preds = le.inverse_transform(preds)\n",
    "df['preds'] = preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
