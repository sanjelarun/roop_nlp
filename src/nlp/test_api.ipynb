{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,TFBertForSequenceClassification\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.nn import softmax\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Dynamically allocate GPU memory\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at model_100000_07_14.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "2023-08-21 15:51:10.578942: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "['map(), take()', 'collect()', 'groupBy()', 'map(), reduce()', 'map()']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,TFBertForSequenceClassification\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.nn import softmax\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Dynamically allocate GPU memory\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "label_encoder = LabelEncoder()\n",
    "num_classes = 111\n",
    "data = []\n",
    "data_set = \"latest_class_10000.csv\"\n",
    "model = TFBertForSequenceClassification.from_pretrained('model_100000_07_14.h5', num_labels=num_classes)\n",
    "with open(data_set, 'r') as csvfile:\n",
    "        for line in csvfile:\n",
    "            if len(line) > 5:\n",
    "                a, b = line.split('\",', 1)\n",
    "                a, b = a[2:], b[2:-2]\n",
    "                bs = re.findall(r\"'(.*?)'\", b, re.DOTALL)\n",
    "                bs = \", \".join(bs)\n",
    "                data.append([a,bs])\n",
    "# data = data[:10]\n",
    "# Separate code from actions\n",
    "code = [item[0] for item in data]\n",
    "actions = [item[1] for item in data]\n",
    "df = pd.DataFrame(list(zip(code, actions)), columns=['code', 'actions'])\n",
    "label_encoder.fit(df['actions'])\n",
    "actions_encoded = label_encoder.transform(df['actions'])\n",
    "action_one_hot = to_categorical(actions_encoded, num_classes = len(set(df['actions'])))\n",
    "action_argmax = np.argmax(action_one_hot)\n",
    "c = code[0]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "first_input_tokenized = tokenizer.encode(c, add_special_tokens=True, max_length=256, truncation=True, padding=\"max_length\")\n",
    "# ind_action_encode = label_encoder.transform(a)\n",
    "# ind_action_one_hot = to_categorical(ind_action_encode, num_classes = num_classes)\n",
    "# ind_act_argmax = np.argmax(ind_action_one_hot)\n",
    "# Make predictions using the model\n",
    "logits = model.predict([first_input_tokenized])\n",
    "\n",
    "# Extract the logits from the TFSequenceClassifierOutput object\n",
    "logits_values = logits.logits\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probs = softmax(logits_values)\n",
    "top_5_indices = np.argsort(probs, axis=1)[:, -5:]\n",
    "# Decode the classes\n",
    "predicted_top_5_labels = [label_encoder.inverse_transform(indices) for indices in top_5_indices]\n",
    "predicted_top_5_labels = [list(indices) for indices in predicted_top_5_labels][0]\n",
    "print(predicted_top_5_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the code\n",
    "input_ids = [tokenizer.encode(c, add_special_tokens=True) for c in df['code']]\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "code_padded = pad_sequences(input_ids, maxlen=256, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.decode(first_input_tokenized) == tokenizer.decode(code_padded[0])\n",
    "assert label_encoder.inverse_transform([action_argmax]) == label_encoder.inverse_transform([ind_act_argmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 166ms/step\n",
      "['collect()', 'groupBy()', 'map(), reduce()', 'map()']\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the model\n",
    "logits = model.predict([first_input_tokenized])\n",
    "\n",
    "# Extract the logits from the TFSequenceClassifierOutput object\n",
    "logits_values = logits.logits\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probs = softmax(logits_values)\n",
    "\n",
    "# Decode the classes\n",
    "predicted_top_5_labels = [label_encoder.inverse_transform(indices) for indices in top_5_indices]\n",
    "predicted_top_5_labels = [list(indices) for indices in predicted_top_5_labels][0]\n",
    "print(predicted_top_5_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 15:43:29.763831: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-08-21 15:43:29.763961: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Some layers from the model checkpoint at model_100000_07_14.h5 were not used when initializing TFBertForSequenceClassification: ['dropout_75']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at model_100000_07_14.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "2023-08-21 15:43:31.431951: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-08-21 15:43:34.239502: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "['map(), take()', 'collect()', 'groupBy()', 'map(), reduce()', 'map()']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.nn import softmax\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "\n",
    "def initialize_gpu():\n",
    "    \"\"\"\n",
    "    Dynamically allocate GPU memory.\n",
    "    \"\"\"\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "def load_dataset(filepath):\n",
    "    \"\"\"\n",
    "    Load the dataset from the given CSV file.\n",
    "    \n",
    "    Args:\n",
    "    - filepath (str): Path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "    - List[Tuple[str, str]]: List of (code, action) pairs.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r') as csvfile:\n",
    "        for line in csvfile:\n",
    "            if len(line) > 5:\n",
    "                a, b = line.split('\",', 1)\n",
    "                a, b = a[2:], b[2:-2]\n",
    "                bs = re.findall(r\"'(.*?)'\", b, re.DOTALL)\n",
    "                bs = \", \".join(bs)\n",
    "                data.append((a,bs))\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_data(data, num_classes):\n",
    "    \"\"\"\n",
    "    Preprocess the data.\n",
    "    \n",
    "    Args:\n",
    "    - data (List[Tuple[str, str]]): List of (code, action) pairs.\n",
    "    - num_classes (int): Number of action classes.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Pandas DataFrame with code and actions columns.\n",
    "    - ndarray: One-hot encoded actions.\n",
    "    \"\"\"\n",
    "    # Separate code from actions\n",
    "    code = [item[0] for item in data]\n",
    "    actions = [item[1] for item in data]\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(code, actions)), columns=['code', 'actions'])\n",
    "    label_encoder.fit(df['actions'])\n",
    "    actions_encoded = label_encoder.transform(df['actions'])\n",
    "    action_one_hot = to_categorical(actions_encoded, num_classes=len(set(df['actions'])))\n",
    "    \n",
    "    return df, action_one_hot\n",
    "\n",
    "\n",
    "def predict_top_5_labels(model, tokenized_input, label_encoder):\n",
    "    \"\"\"\n",
    "    Predict the top 5 labels for a tokenized input.\n",
    "    \n",
    "    Args:\n",
    "    - model (TFBertForSequenceClassification): The BERT model.\n",
    "    - tokenized_input (List[int]): Tokenized input sequence.\n",
    "    - label_encoder (LabelEncoder): Encoder for action labels.\n",
    "    \n",
    "    Returns:\n",
    "    - List[str]: Top 5 predicted labels.\n",
    "    \"\"\"\n",
    "    logits = model.predict([tokenized_input])\n",
    "    logits_values = logits.logits\n",
    "    probs = softmax(logits_values)\n",
    "    \n",
    "    top_5_indices = np.argsort(probs, axis=1)[:, -5:]\n",
    "    predicted_top_5_labels = [label_encoder.inverse_transform(indices) for indices in top_5_indices]\n",
    "    return [list(indices) for indices in predicted_top_5_labels][0]\n",
    "\n",
    "\n",
    "# Initialization\n",
    "np.random.seed(42)\n",
    "initialize_gpu()\n",
    "label_encoder = LabelEncoder()\n",
    "num_classes = 111\n",
    "\n",
    "# Load the model and data\n",
    "model = TFBertForSequenceClassification.from_pretrained('model_100000_07_14.h5', num_labels=num_classes)\n",
    "data = load_dataset(\"latest_class_10000.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "df, action_one_hot = preprocess_data(data, num_classes)\n",
    "\n",
    "# Tokenize a sample input\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "first_input_tokenized = tokenizer.encode(df['code'][0], add_special_tokens=True, max_length=256, truncation=True, padding=\"max_length\")\n",
    "\n",
    "# Predict the top 5 labels\n",
    "top_5_labels = predict_top_5_labels(model, first_input_tokenized, label_encoder)\n",
    "print(top_5_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at model_100000_07_14.h5 were not used when initializing TFBertForSequenceClassification: ['dropout_75']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at model_100000_07_14.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "2023-08-22 09:46:02.495954: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 1s 560ms/step\n",
      "['sortBy()', 'map(), count()', 'sum()', 'collect()', 'count()']\n",
      "count()\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.nn import softmax\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "\n",
    "class BertClassifier:\n",
    "    def __init__(self, model_path, num_classes):\n",
    "        self.model = TFBertForSequenceClassification.from_pretrained(model_path, num_labels=num_classes)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.initialize_gpu()\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_gpu():\n",
    "        \"\"\"Dynamically allocate GPU memory.\"\"\"\n",
    "        physical_devices = tf.config.list_physical_devices('GPU')\n",
    "        if physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dataset(filepath):\n",
    "        \"\"\"Load the dataset from the given CSV file.\"\"\"\n",
    "        data = []\n",
    "        with open(filepath, 'r') as csvfile:\n",
    "            for line in csvfile:\n",
    "                if len(line) > 5:\n",
    "                    a, b = line.split('\",', 1)\n",
    "                    a, b = a[2:], b[2:-2]\n",
    "                    bs = re.findall(r\"'(.*?)'\", b, re.DOTALL)\n",
    "                    bs = \", \".join(bs)\n",
    "                    data.append((a, bs))\n",
    "        return data\n",
    "\n",
    "    def preprocess_data(self, data, num_classes):\n",
    "        \"\"\"Preprocess the data.\"\"\"\n",
    "        code = [item[0] for item in data]\n",
    "        actions = [item[1] for item in data]\n",
    "\n",
    "        df = pd.DataFrame(list(zip(code, actions)), columns=['code', 'actions'])\n",
    "        return df\n",
    "\n",
    "    def fit_label(self, df):\n",
    "        self.label_encoder.fit(df['actions'])\n",
    "\n",
    "    def predict_top_5_labels(self, code):\n",
    "        \"\"\"Predict the top 5 labels for a given code.\"\"\"\n",
    "        tokenized_input = self.tokenizer.encode(code, add_special_tokens=True, max_length=256, truncation=True, padding=\"max_length\")\n",
    "        logits = self.model.predict([tokenized_input])\n",
    "        logits_values = logits.logits\n",
    "        probs = softmax(logits_values)\n",
    "\n",
    "        top_5_indices = np.argsort(probs, axis=1)[:, -5:]\n",
    "        predicted_top_5_labels = [self.label_encoder.inverse_transform(indices) for indices in top_5_indices]\n",
    "        return [list(indices) for indices in predicted_top_5_labels][0]\n",
    "\n",
    "    def predict_top_class(self,code):\n",
    "        tokenized_input = self.tokenizer.encode(code, add_special_tokens=True, max_length=256, truncation=True, padding=\"max_length\")\n",
    "        logits = self.model.predict([tokenized_input])\n",
    "        logits_values = logits.logits\n",
    "        probs = softmax(logits_values)\n",
    "        # Find the class with the maximum probability\n",
    "        predicted_classes = np.argmax(probs, axis=1)\n",
    "        predicted_classes = self.label_encoder.inverse_transform(predicted_classes)\n",
    "        return predicted_classes[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Top5Predictions:\n",
    "    def __init__(self):\n",
    "        self.classifier = BertClassifier('model_100000_07_14.h5', 111)\n",
    "        data = self.classifier.load_dataset(\"latest_class_10000.csv\")\n",
    "        df = self.classifier.preprocess_data(data, 111)\n",
    "        self.classifier.fit_label(df)\n",
    "    \n",
    "    def make_prediction(self, code):\n",
    "        return self.classifier.predict_top_5_labels(code)\n",
    "\n",
    "top5 = Top5Predictions()\n",
    "code_snap = \"for num in numbers:\\\n",
    "count += 1\"\n",
    "top_5_labels = top5.make_prediction(code_snap)\n",
    "top_class = top5.classifier.predict_top_class(code_snap)\n",
    "print(top_5_labels)\n",
    "print(top_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at model_100000_07_14.h5 were not used when initializing TFBertForSequenceClassification: ['dropout_75']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at model_100000_07_14.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "2023-08-21 16:06:31.510288: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x3c5ca91b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "['map(), count()', 'map(), reduce()', 'groupBy()', 'collect()', 'map()']\n"
     ]
    }
   ],
   "source": [
    "top5 = Top5Predictions()\n",
    "code_snap = \"for i in range(len(data0)):\\n\\tdata[i] *= 3\"\n",
    "top_5_labels = top5.make_prediction(code_snap)\n",
    "print(top_5_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
